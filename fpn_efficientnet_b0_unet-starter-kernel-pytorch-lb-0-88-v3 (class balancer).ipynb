{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet starter for Steel defect detection challenge\n",
    "\n",
    "\n",
    "This kernel uses a UNet model with pretrained resnet18 encoder for this challenge, with simple augmentations using albumentations library, uses BCE loss, metrics like Dice and IoU. I've used [segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch) which comes with a lot pre-implemented segmentation architectures. This is a modified version of my previous [kernel](https://www.kaggle.com/rishabhiitbhu/unet-with-resnet34-encoder-pytorch) for [siim-acr-pneumothorax-segmentation](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/) competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As internet is not allowed for this competition, I tried installing `segmentation_models.pytorch` by source using pip but due to some reasons it didn't work. So, as a [Jugaad](https://en.wikipedia.org/wiki/Jugaad) I took all of `segmentation_models.pytorch`'s UNet code and wrote it in a single file and added it as a dataset so as to use it for this kernel, its dependency [pretrained-models.pytorch](https://github.com/Cadene/pretrained-models.pytorch) is also added as a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install git+https://github.com/qubvel/segmentation_models.pytorch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_name = 'efficientnet-b0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# !pip install ../input/pretrainedmodels/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4/ > /dev/null # no output\n",
    "# package_path = './unetmodelscript/' # add unet script dataset\n",
    "# import sys\n",
    "# sys.path.append(package_path)\n",
    "# from model import Unet # import Unet model from the script\n",
    "import segmentation_models_pytorch as smp #import Unet, FPN, PSPNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "import cv2\n",
    "import pdb\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "from matplotlib import pyplot as plt\n",
    "from albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)\n",
    "from albumentations.torch import ToTensor\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "seed = 69\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "from radam import RAdam\n",
    "\n",
    "from torchcontrib.optim import SWA\n",
    "from torch.utils.data import Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLE-Mask utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\n",
    "def mask2rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 -> mask, 0 -> background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def make_mask(row_id, df):\n",
    "    '''Given a row index, return image_id and mask (256, 1600, 4) from the dataframe `df`'''\n",
    "    fname = df.iloc[row_id].name\n",
    "    labels = df.iloc[row_id][:4]\n",
    "    masks = np.zeros((256, 1600, 4), dtype=np.float32) # float32 is V.Imp\n",
    "    # 4:class 1～4 (ch:0～3)\n",
    "\n",
    "    for idx, label in enumerate(labels.values):\n",
    "        if label is not np.nan:\n",
    "            label = label.split(\" \")\n",
    "            positions = map(int, label[0::2])\n",
    "            length = map(int, label[1::2])\n",
    "            mask = np.zeros(256 * 1600, dtype=np.uint8)\n",
    "            for pos, le in zip(positions, length):\n",
    "                mask[pos:(pos + le)] = 1\n",
    "            masks[:, :, idx] = mask.reshape(256, 1600, order='F')\n",
    "    return fname, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteelDataset(Dataset):\n",
    "    def __init__(self, df, data_folder, mean, std, phase):\n",
    "        self.df = df\n",
    "        self.root = data_folder\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.phase = phase\n",
    "        self.transforms = get_transforms(phase, mean, std)\n",
    "        self.fnames = self.df.index.tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id, mask = make_mask(idx, self.df)\n",
    "        image_path = os.path.join(self.root, \"train_images\",  image_id)\n",
    "        img = cv2.imread(image_path)\n",
    "        augmented = self.transforms(image=img, mask=mask)\n",
    "        img = augmented['image']\n",
    "        mask = augmented['mask'] # 1x256x1600x4\n",
    "        mask = mask[0].permute(2, 0, 1) # 1x4x256x1600\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "\n",
    "\n",
    "def get_transforms(phase, mean, std):\n",
    "    list_transforms = []\n",
    "    if phase == \"train\":\n",
    "        list_transforms.extend(\n",
    "            [\n",
    "                HorizontalFlip(p=0.5), # only horizontal flip as of now\n",
    "            ]\n",
    "        )\n",
    "    list_transforms.extend(\n",
    "        [\n",
    "            Normalize(mean=mean, std=std, p=1),\n",
    "            ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    list_trfms = Compose(list_transforms)\n",
    "    return list_trfms\n",
    "\n",
    "class FiveBalanceClassSampler(Sampler):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        # label = (self.dataset.df['Label'].values)\n",
    "        # label = label.reshape(-1,4)\n",
    "        # label = np.hstack([label.sum(1,keepdims=True)==0,label]).T\n",
    "\n",
    "        # self.neg_index  = np.where(label[0])[0]\n",
    "        # self.pos1_index = np.where(label[1])[0]\n",
    "        # self.pos2_index = np.where(label[2])[0]\n",
    "        # self.pos3_index = np.where(label[3])[0]\n",
    "        # self.pos4_index = np.where(label[4])[0]\n",
    "        \n",
    "        self.neg_index = np.where(self.dataset.df['defects'] == 0)[0]\n",
    "        self.pos1_index = np.where(~ pd.isna(self.dataset.df['class_1']))[0]\n",
    "        self.pos2_index = np.where(~ pd.isna(self.dataset.df['class_2']))[0]\n",
    "        self.pos3_index = np.where(~ pd.isna(self.dataset.df['class_3']))[0]\n",
    "        self.pos4_index = np.where(~ pd.isna(self.dataset.df['class_4']))[0]\n",
    "\n",
    "        #5x\n",
    "        self.num_image = len(self.dataset.df)//4\n",
    "        self.length = self.num_image*5\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        # neg = self.neg_index.copy()\n",
    "        # random.shuffle(neg)\n",
    "\n",
    "        neg  = np.random.choice(self.neg_index,  self.num_image, replace=True)\n",
    "        pos1 = np.random.choice(self.pos1_index, self.num_image, replace=True)\n",
    "        pos2 = np.random.choice(self.pos2_index, self.num_image, replace=True)\n",
    "        pos3 = np.random.choice(self.pos3_index, self.num_image, replace=True)\n",
    "        pos4 = np.random.choice(self.pos4_index, self.num_image, replace=True)\n",
    "    \n",
    "        l = np.stack([neg,pos1,pos2,pos3,pos4]).T\n",
    "        l = l.reshape(-1)\n",
    "        return iter(l)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "def provider(\n",
    "    data_folder,\n",
    "    df_path,\n",
    "    phase,\n",
    "    mean=None,\n",
    "    std=None,\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "):\n",
    "    '''Returns dataloader for the model training'''\n",
    "    df = pd.read_csv(df_path)\n",
    "    uid = list(df['ImageId_ClassId'].str.split('_').str[0].unique())\n",
    "    df['Class'] = df['ImageId_ClassId'].str[-1].astype(np.int32)\n",
    "    df['Label'] = (df['EncodedPixels']!='').astype(np.int32)\n",
    "    df = df_loc_by_list(df, 'ImageId_ClassId', \n",
    "                        [ u.split('/')[-1] + '_%d'%c  for u in uid for c in [1,2,3,4] ])\n",
    "    \n",
    "    \n",
    "    # https://www.kaggle.com/amanooo/defect-detection-starter-u-net\n",
    "    df['ImageId'], df['ClassId'] = zip(*df['ImageId_ClassId'].str.split('_'))\n",
    "    df['ClassId'] = df['ClassId'].astype(int)\n",
    "    df = df.pivot(index='ImageId',columns='ClassId',values='EncodedPixels')\n",
    "    \n",
    "    # added by Frank\n",
    "    # df.reset_index(inplace=True)\n",
    "    df = df.rename(columns={df.columns[0]: \"class_1\",\n",
    "                            df.columns[1]: \"class_2\",\n",
    "                            df.columns[2]: \"class_3\",\n",
    "                            df.columns[3]: \"class_4\"})\n",
    "\n",
    "    df['defects'] = df.count(axis=1)\n",
    "    \n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"defects\"], random_state=69)\n",
    "    df = train_df if phase == \"train\" else val_df\n",
    "    image_dataset = SteelDataset(df, data_folder, mean, std, phase)\n",
    "    dataloader = DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        # shuffle=True,   \n",
    "        sampler=FiveBalanceClassSampler(image_dataset)\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def df_loc_by_list(df, key, values):\n",
    "#     df = df.loc[df[key].isin(values)]\n",
    "#     df = df.assign(sort = pd.Categorical(df[key], categories=values, ordered=True))\n",
    "#     df = df.sort_values('sort')\n",
    "#     #df = df.reset_index()\n",
    "#     df = df.drop('sort', axis=1)\n",
    "#     return  df\n",
    "\n",
    "# df = pd.read_csv('/home/frank/Dropbox/Project/kaggle_severstal/input/train.csv')\n",
    "# uid = list(df['ImageId_ClassId'].str.split('_').str[0].unique())\n",
    "# df['Class'] = df['ImageId_ClassId'].str[-1].astype(np.int32)\n",
    "# df['Label'] = (df['EncodedPixels']!='').astype(np.int32)\n",
    "# df = df_loc_by_list(df, 'ImageId_ClassId', \n",
    "#                     [ u.split('/')[-1] + '_%d'%c  for u in uid for c in [1,2,3,4] ])\n",
    "\n",
    "\n",
    "# # https://www.kaggle.com/amanooo/defect-detection-starter-u-net\n",
    "# df['ImageId'], df['ClassId'] = zip(*df['ImageId_ClassId'].str.split('_'))\n",
    "# df['ClassId'] = df['ClassId'].astype(int)\n",
    "# df = df.pivot(index='ImageId',columns='ClassId',values='EncodedPixels')\n",
    "\n",
    "# # added by Frank\n",
    "# # df.reset_index(inplace=True)\n",
    "# df = df.rename(columns={df.columns[0]: \"class_1\",\n",
    "#                         df.columns[1]: \"class_2\",\n",
    "#                         df.columns[2]: \"class_3\",\n",
    "#                         df.columns[3]: \"class_4\"})\n",
    "\n",
    "# df['defects'] = df.count(axis=1)\n",
    "\n",
    "# train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"defects\"], random_state=69)\n",
    "\n",
    "# phase = 'train'\n",
    "# mean=(0.485, 0.456, 0.406),\n",
    "# std=(0.229, 0.224, 0.225)\n",
    "# data_folder = '/home/frank/Dropbox/Project/kaggle_severstal/input/'\n",
    "# df = train_df if phase == \"train\" else val_df\n",
    "# image_dataset = SteelDataset(df, data_folder, mean, std, phase)\n",
    "# len(image_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/home/frank/Dropbox/Project/kaggle_severstal/input/train.csv')\n",
    "# # uid = list(df['ImageId_ClassId'].str.split('_').str[0].unique())\n",
    "# # df['Class'] = df['ImageId_ClassId'].str[-1].astype(np.int32)\n",
    "# # df['Label'] = (df['EncodedPixels']!='').astype(np.int32)\n",
    "# # df = df_loc_by_list(df, 'ImageId_ClassId', \n",
    "# #                     [ u.split('/')[-1] + '_%d'%c  for u in uid for c in [1,2,3,4] ])\n",
    "\n",
    "\n",
    "# # https://www.kaggle.com/amanooo/defect-detection-starter-u-net\n",
    "# df['ImageId'], df['ClassId'] = zip(*df['ImageId_ClassId'].str.split('_'))\n",
    "# df['ClassId'] = df['ClassId'].astype(int)\n",
    "# df = df.pivot(index='ImageId',columns='ClassId',values='EncodedPixels')\n",
    "# # df.reset_index(inplace=True)\n",
    "# # df = df.rename(columns={df.columns[1]: \"class_1\",\n",
    "# #                         df.columns[2]: \"class_2\",\n",
    "# #                         df.columns[3]: \"class_3\",\n",
    "# #                         df.columns[4]: \"class_4\"})\n",
    "# df = df.rename(columns={df.columns[1]: \"class_1\",\n",
    "#                         df.columns[2]: \"class_2\",\n",
    "#                         df.columns[3]: \"class_3\"})\n",
    "# df['defects'] = df.count(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.read_csv('/home/frank/Dropbox/Project/kaggle_severstal/input/train.csv')\n",
    "# uid = list(df2['ImageId_ClassId'].str.split('_').str[0].unique())\n",
    "# df2['Class'] = df2['ImageId_ClassId'].str[-1].astype(np.int32)\n",
    "# df2['Label'] = (df2['EncodedPixels']!='').astype(np.int32)\n",
    "# df2 = df_loc_by_list(df2, 'ImageId_ClassId', \n",
    "#                     [ u.split('/')[-1] + '_%d'%c  for u in uid for c in [1,2,3,4] ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more utility functions\n",
    "\n",
    "Dice and IoU metric implementations, metric logger for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_loc_by_list(df, key, values):\n",
    "    df = df.loc[df[key].isin(values)]\n",
    "    df = df.assign(sort = pd.Categorical(df[key], categories=values, ordered=True))\n",
    "    df = df.sort_values('sort')\n",
    "    #df = df.reset_index()\n",
    "    df = df.drop('sort', axis=1)\n",
    "    return  df\n",
    "\n",
    "def predict(X, threshold):\n",
    "    '''X is sigmoid output of the model'''\n",
    "    X_p = np.copy(X)\n",
    "    preds = (X_p > threshold).astype('uint8')\n",
    "    return preds\n",
    "\n",
    "def metric(probability, truth, threshold=0.5, reduction='none'):\n",
    "    '''Calculates dice of positive and negative images seperately'''\n",
    "    '''probability and truth must be torch tensors'''\n",
    "    batch_size = len(truth)\n",
    "    with torch.no_grad():\n",
    "        probability = probability.view(batch_size, -1)\n",
    "        truth = truth.view(batch_size, -1)\n",
    "        assert(probability.shape == truth.shape)\n",
    "\n",
    "        p = (probability > threshold).float()\n",
    "        t = (truth > 0.5).float()\n",
    "\n",
    "        t_sum = t.sum(-1)\n",
    "        p_sum = p.sum(-1)\n",
    "        neg_index = torch.nonzero(t_sum == 0)\n",
    "        pos_index = torch.nonzero(t_sum >= 1)\n",
    "\n",
    "        dice_neg = (p_sum == 0).float()\n",
    "        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n",
    "\n",
    "        dice_neg = dice_neg[neg_index]\n",
    "        dice_pos = dice_pos[pos_index]\n",
    "        dice = torch.cat([dice_pos, dice_neg])\n",
    "\n",
    "        dice_neg = np.nan_to_num(dice_neg.mean().item(), 0)\n",
    "        dice_pos = np.nan_to_num(dice_pos.mean().item(), 0)\n",
    "        dice = dice.mean().item()\n",
    "\n",
    "        num_neg = len(neg_index)\n",
    "        num_pos = len(pos_index)\n",
    "\n",
    "    return dice, dice_neg, dice_pos, num_neg, num_pos\n",
    "\n",
    "class Meter:\n",
    "    '''A meter to keep track of iou and dice scores throughout an epoch'''\n",
    "    def __init__(self, phase, epoch):\n",
    "        self.base_threshold = 0.5 # <<<<<<<<<<< here's the threshold\n",
    "        self.base_dice_scores = []\n",
    "        self.dice_neg_scores = []\n",
    "        self.dice_pos_scores = []\n",
    "        self.iou_scores = []\n",
    "\n",
    "    def update(self, targets, outputs):\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        dice, dice_neg, dice_pos, _, _ = metric(probs, targets, self.base_threshold)\n",
    "        self.base_dice_scores.append(dice)\n",
    "        self.dice_pos_scores.append(dice_pos)\n",
    "        self.dice_neg_scores.append(dice_neg)\n",
    "        preds = predict(probs, self.base_threshold)\n",
    "        iou = compute_iou_batch(preds, targets, classes=[1])\n",
    "        self.iou_scores.append(iou)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        dice = np.mean(self.base_dice_scores)\n",
    "        dice_neg = np.mean(self.dice_neg_scores)\n",
    "        dice_pos = np.mean(self.dice_pos_scores)\n",
    "        dices = [dice, dice_neg, dice_pos]\n",
    "        iou = np.nanmean(self.iou_scores)\n",
    "        return dices, iou\n",
    "\n",
    "def epoch_log(phase, epoch, epoch_loss, meter, start):\n",
    "    '''logging the metrics at the end of an epoch'''\n",
    "    dices, iou = meter.get_metrics()\n",
    "    dice, dice_neg, dice_pos = dices\n",
    "    print(\"Loss: %0.4f | IoU: %0.4f | dice: %0.4f | dice_neg: %0.4f | dice_pos: %0.4f\" % (epoch_loss, iou, dice, dice_neg, dice_pos))\n",
    "    return dice, iou\n",
    "\n",
    "def compute_ious(pred, label, classes, ignore_index=255, only_present=True):\n",
    "    '''computes iou for one ground truth mask and predicted mask'''\n",
    "    pred[label == ignore_index] = 0\n",
    "    ious = []\n",
    "    for c in classes:\n",
    "        label_c = label == c\n",
    "        if only_present and np.sum(label_c) == 0:\n",
    "            ious.append(np.nan)\n",
    "            continue\n",
    "        pred_c = pred == c\n",
    "        intersection = np.logical_and(pred_c, label_c).sum()\n",
    "        union = np.logical_or(pred_c, label_c).sum()\n",
    "        if union != 0:\n",
    "            ious.append(intersection / union)\n",
    "    return ious if ious else [1]\n",
    "\n",
    "def compute_iou_batch(outputs, labels, classes=None):\n",
    "    '''computes mean iou for a batch of ground truth masks and predicted masks'''\n",
    "    ious = []\n",
    "    preds = np.copy(outputs) # copy is imp\n",
    "    labels = np.array(labels) # tensor to np\n",
    "    for pred, label in zip(preds, labels):\n",
    "        ious.append(np.nanmean(compute_ious(pred, label, classes)))\n",
    "    iou = np.nanmean(ious)\n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p /tmp/.cache/torch/checkpoints/\n",
    "# !cp ../input/resnet18/resnet18.pth /tmp/.cache/torch/checkpoints/resnet18-5c106cde.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=4, activation=None)\n",
    "model = smp.FPN(encoder_name, encoder_weights=\"imagenet\", classes=4, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [EarlyStopping(monitor='val_loss', patience=2)]\n",
    "# model.set_callbacks(callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FPN(\n",
       "  (encoder): EfficientNetEncoder(\n",
       "    (_conv_stem): Conv2dStaticSamePadding(\n",
       "      3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "      (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "    )\n",
       "    (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_blocks): ModuleList(\n",
       "      (0): MBConvBlock(\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (6): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (7): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (8): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (9): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (10): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (11): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (12): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (13): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (14): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (15): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (_conv_head): Conv2dStaticSamePadding(\n",
       "      320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (static_padding): Identity()\n",
       "    )\n",
       "    (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (decoder): FPNDecoder(\n",
       "    (conv1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (p4): FPNBlock(\n",
       "      (skip_conv): Conv2d(112, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (p3): FPNBlock(\n",
       "      (skip_conv): Conv2d(40, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (p2): FPNBlock(\n",
       "      (skip_conv): Conv2d(24, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (s5): SegmentationBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv3x3GNReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Conv3x3GNReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Conv3x3GNReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (s4): SegmentationBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv3x3GNReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Conv3x3GNReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (s3): SegmentationBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv3x3GNReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (s2): SegmentationBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv3x3GNReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout2d(p=0.2, inplace=True)\n",
       "    (final_conv): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model # a *deeper* look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    '''This class takes care of training and validation of our model'''\n",
    "    def __init__(self, model):\n",
    "        self.num_workers = 3\n",
    "        self.batch_size = {\"train\": 8, \"val\": 8}\n",
    "        self.accumulation_steps = 32 // self.batch_size['train']\n",
    "        self.lr = 5e-4\n",
    "        self.num_epochs = 200\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.phases = [\"train\", \"val\"]\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "        self.net = model\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        # self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.optimizer = RAdam(self.net.parameters(), lr=self.lr)\n",
    "#         base_opt = optim.SGD(self.net.parameters(), \n",
    "#                              lr=self.lr,\n",
    "#                              momentum=0.9, \n",
    "#                              weight_decay=0.0001)\n",
    "#         self.optimizer = SWA(base_opt, swa_start=15, \n",
    "#                              swa_freq=5, swa_lr=2.5e-4)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", patience=3, verbose=True)\n",
    "        self.net = self.net.to(self.device)\n",
    "        cudnn.benchmark = True\n",
    "        self.dataloaders = {\n",
    "            phase: provider(\n",
    "                data_folder=data_folder,\n",
    "                df_path=train_df_path,\n",
    "                phase=phase,\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225),\n",
    "                batch_size=self.batch_size[phase],\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "            for phase in self.phases\n",
    "        }\n",
    "        self.losses = {phase: [] for phase in self.phases}\n",
    "        self.iou_scores = {phase: [] for phase in self.phases}\n",
    "        self.dice_scores = {phase: [] for phase in self.phases}\n",
    "        \n",
    "    def forward(self, images, targets):\n",
    "        images = images.to(self.device)\n",
    "        masks = targets.to(self.device)\n",
    "        outputs = self.net(images)\n",
    "        loss = self.criterion(outputs, masks)\n",
    "        return loss, outputs\n",
    "\n",
    "    def iterate(self, epoch, phase):\n",
    "        meter = Meter(phase, epoch)\n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        print(f\"Starting epoch: {epoch} | phase: {phase} | ⏰: {start}\")\n",
    "        batch_size = self.batch_size[phase]\n",
    "        self.net.train(phase == \"train\")\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        running_loss = 0.0\n",
    "        total_batches = len(dataloader)\n",
    "#         tk0 = tqdm(dataloader, total=total_batches)\n",
    "        self.optimizer.zero_grad()\n",
    "        for itr, batch in enumerate(dataloader): # replace `dataloader` with `tk0` for tqdm\n",
    "            images, targets = batch\n",
    "            loss, outputs = self.forward(images, targets)\n",
    "            loss = loss / self.accumulation_steps\n",
    "            if phase == \"train\":\n",
    "                loss.backward()\n",
    "                if (itr + 1 ) % self.accumulation_steps == 0:\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "            outputs = outputs.detach().cpu()\n",
    "            meter.update(targets, outputs)\n",
    "#             tk0.set_postfix(loss=(running_loss / ((itr + 1))))\n",
    "        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n",
    "        dice, iou = epoch_log(phase, epoch, epoch_loss, meter, start)\n",
    "        self.losses[phase].append(epoch_loss)\n",
    "        self.dice_scores[phase].append(dice)\n",
    "        self.iou_scores[phase].append(iou)\n",
    "        torch.cuda.empty_cache()\n",
    "        return epoch_loss\n",
    "\n",
    "    def start(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.iterate(epoch, \"train\")\n",
    "#             if epoch+1 == self.num_epochs:\n",
    "#                 self.optimizer.swap_swa_sgd()\n",
    "            state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"best_loss\": self.best_loss,\n",
    "                \"state_dict\": self.net.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict(),\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                val_loss = self.iterate(epoch, \"val\")\n",
    "                self.scheduler.step(val_loss)\n",
    "            if val_loss < self.best_loss:\n",
    "                print(\"******** New optimal found, saving state ********\")\n",
    "                state[\"best_loss\"] = self.best_loss = val_loss\n",
    "                torch.save(state, \"./\"+encoder_name+\"_RAdam_classBal_bestModel.pth\")\n",
    "            torch.save(state, \"./\"+encoder_name+\"_RAdam_classBal_model_epoch_\"+str(epoch)+\".pth\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_path = '../input/sample_submission.csv'\n",
    "train_df_path = '../input/train.csv'\n",
    "data_folder = \"../input/\"\n",
    "test_data_folder = \"../input/test_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: train | ⏰: 23:46:59\n",
      "Loss: 0.0991 | IoU: 0.1535 | dice: 0.2064 | dice_neg: 0.1677 | dice_pos: 0.2171\n",
      "Starting epoch: 0 | phase: val | ⏰: 23:58:34\n",
      "Loss: 0.0180 | IoU: 0.3033 | dice: 0.4598 | dice_neg: 0.6349 | dice_pos: 0.4154\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 1 | phase: train | ⏰: 00:00:31\n",
      "Loss: 0.0174 | IoU: 0.3438 | dice: 0.4888 | dice_neg: 0.6025 | dice_pos: 0.4629\n",
      "Starting epoch: 1 | phase: val | ⏰: 00:11:56\n",
      "Loss: 0.0151 | IoU: 0.4455 | dice: 0.6118 | dice_neg: 0.7366 | dice_pos: 0.5812\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 2 | phase: train | ⏰: 00:13:49\n",
      "Loss: 0.0142 | IoU: 0.4251 | dice: 0.5941 | dice_neg: 0.7651 | dice_pos: 0.5545\n",
      "Starting epoch: 2 | phase: val | ⏰: 00:24:40\n",
      "Loss: 0.0141 | IoU: 0.4724 | dice: 0.6554 | dice_neg: 0.8321 | dice_pos: 0.6101\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 3 | phase: train | ⏰: 00:26:18\n",
      "Loss: 0.0120 | IoU: 0.4751 | dice: 0.6476 | dice_neg: 0.8113 | dice_pos: 0.6078\n",
      "Starting epoch: 3 | phase: val | ⏰: 00:37:15\n",
      "Loss: 0.0137 | IoU: 0.4853 | dice: 0.6679 | dice_neg: 0.8677 | dice_pos: 0.6177\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 4 | phase: train | ⏰: 00:38:54\n",
      "Loss: 0.0107 | IoU: 0.5203 | dice: 0.6900 | dice_neg: 0.8453 | dice_pos: 0.6527\n",
      "Starting epoch: 4 | phase: val | ⏰: 00:49:43\n",
      "Loss: 0.0145 | IoU: 0.4843 | dice: 0.6758 | dice_neg: 0.9008 | dice_pos: 0.6199\n",
      "\n",
      "Starting epoch: 5 | phase: train | ⏰: 00:51:21\n",
      "Loss: 0.0098 | IoU: 0.5527 | dice: 0.7223 | dice_neg: 0.8829 | dice_pos: 0.6845\n",
      "Starting epoch: 5 | phase: val | ⏰: 01:02:05\n",
      "Loss: 0.0163 | IoU: 0.4900 | dice: 0.6789 | dice_neg: 0.9237 | dice_pos: 0.6194\n",
      "\n",
      "Starting epoch: 6 | phase: train | ⏰: 01:03:42\n",
      "Loss: 0.0088 | IoU: 0.5803 | dice: 0.7435 | dice_neg: 0.8905 | dice_pos: 0.7088\n",
      "Starting epoch: 6 | phase: val | ⏰: 01:14:27\n",
      "Loss: 0.0146 | IoU: 0.5081 | dice: 0.6907 | dice_neg: 0.8842 | dice_pos: 0.6397\n",
      "\n",
      "Starting epoch: 7 | phase: train | ⏰: 01:16:04\n",
      "Loss: 0.0088 | IoU: 0.5880 | dice: 0.7470 | dice_neg: 0.8740 | dice_pos: 0.7170\n",
      "Starting epoch: 7 | phase: val | ⏰: 01:26:49\n",
      "Loss: 0.0157 | IoU: 0.5128 | dice: 0.6820 | dice_neg: 0.8168 | dice_pos: 0.6463\n",
      "Epoch     7: reducing learning rate of group 0 to 5.0000e-05.\n",
      "\n",
      "Starting epoch: 8 | phase: train | ⏰: 01:28:26\n",
      "Loss: 0.0076 | IoU: 0.6246 | dice: 0.7796 | dice_neg: 0.9153 | dice_pos: 0.7486\n",
      "Starting epoch: 8 | phase: val | ⏰: 01:39:11\n",
      "Loss: 0.0147 | IoU: 0.5219 | dice: 0.7074 | dice_neg: 0.9173 | dice_pos: 0.6546\n",
      "\n",
      "Starting epoch: 9 | phase: train | ⏰: 01:40:48\n",
      "Loss: 0.0070 | IoU: 0.6368 | dice: 0.7853 | dice_neg: 0.8985 | dice_pos: 0.7585\n",
      "Starting epoch: 9 | phase: val | ⏰: 01:51:32\n",
      "Loss: 0.0148 | IoU: 0.5251 | dice: 0.7133 | dice_neg: 0.9300 | dice_pos: 0.6583\n",
      "\n",
      "Starting epoch: 10 | phase: train | ⏰: 01:53:10\n",
      "Loss: 0.0070 | IoU: 0.6456 | dice: 0.7973 | dice_neg: 0.9198 | dice_pos: 0.7676\n",
      "Starting epoch: 10 | phase: val | ⏰: 02:03:59\n",
      "Loss: 0.0146 | IoU: 0.5193 | dice: 0.7055 | dice_neg: 0.9173 | dice_pos: 0.6521\n",
      "\n",
      "Starting epoch: 11 | phase: train | ⏰: 02:05:38\n",
      "Loss: 0.0067 | IoU: 0.6501 | dice: 0.8012 | dice_neg: 0.9268 | dice_pos: 0.7704\n",
      "Starting epoch: 11 | phase: val | ⏰: 02:16:20\n",
      "Loss: 0.0155 | IoU: 0.5287 | dice: 0.7093 | dice_neg: 0.8969 | dice_pos: 0.6614\n",
      "Epoch    11: reducing learning rate of group 0 to 5.0000e-06.\n",
      "\n",
      "Starting epoch: 12 | phase: train | ⏰: 02:17:57\n",
      "Loss: 0.0065 | IoU: 0.6571 | dice: 0.8054 | dice_neg: 0.9262 | dice_pos: 0.7768\n",
      "Starting epoch: 12 | phase: val | ⏰: 02:28:39\n",
      "Loss: 0.0152 | IoU: 0.5314 | dice: 0.7162 | dice_neg: 0.9288 | dice_pos: 0.6640\n",
      "\n",
      "Starting epoch: 13 | phase: train | ⏰: 02:30:16\n",
      "Loss: 0.0065 | IoU: 0.6610 | dice: 0.8087 | dice_neg: 0.9328 | dice_pos: 0.7796\n",
      "Starting epoch: 13 | phase: val | ⏰: 02:41:04\n",
      "Loss: 0.0137 | IoU: 0.5330 | dice: 0.7187 | dice_neg: 0.9211 | dice_pos: 0.6654\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 14 | phase: train | ⏰: 02:42:45\n",
      "Loss: 0.0063 | IoU: 0.6587 | dice: 0.8053 | dice_neg: 0.9227 | dice_pos: 0.7773\n",
      "Starting epoch: 14 | phase: val | ⏰: 02:53:31\n",
      "Loss: 0.0152 | IoU: 0.5300 | dice: 0.7165 | dice_neg: 0.9262 | dice_pos: 0.6629\n",
      "\n",
      "Starting epoch: 15 | phase: train | ⏰: 02:55:07\n",
      "Loss: 0.0064 | IoU: 0.6566 | dice: 0.8058 | dice_neg: 0.9297 | dice_pos: 0.7755\n",
      "Starting epoch: 15 | phase: val | ⏰: 03:06:01\n",
      "Loss: 0.0145 | IoU: 0.5375 | dice: 0.7248 | dice_neg: 0.9326 | dice_pos: 0.6705\n",
      "\n",
      "Starting epoch: 16 | phase: train | ⏰: 03:07:39\n",
      "Loss: 0.0064 | IoU: 0.6608 | dice: 0.8093 | dice_neg: 0.9297 | dice_pos: 0.7798\n",
      "Starting epoch: 16 | phase: val | ⏰: 03:18:25\n",
      "Loss: 0.0143 | IoU: 0.5340 | dice: 0.7188 | dice_neg: 0.9198 | dice_pos: 0.6676\n",
      "\n",
      "Starting epoch: 17 | phase: train | ⏰: 03:20:09\n",
      "Loss: 0.0063 | IoU: 0.6593 | dice: 0.8080 | dice_neg: 0.9348 | dice_pos: 0.7779\n",
      "Starting epoch: 17 | phase: val | ⏰: 03:31:06\n",
      "Loss: 0.0149 | IoU: 0.5329 | dice: 0.7162 | dice_neg: 0.9211 | dice_pos: 0.6641\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0000e-07.\n",
      "\n",
      "Starting epoch: 18 | phase: train | ⏰: 03:32:44\n",
      "Loss: 0.0062 | IoU: 0.6613 | dice: 0.8087 | dice_neg: 0.9274 | dice_pos: 0.7797\n",
      "Starting epoch: 18 | phase: val | ⏰: 03:43:28\n",
      "Loss: 0.0151 | IoU: 0.5272 | dice: 0.7120 | dice_neg: 0.9326 | dice_pos: 0.6565\n",
      "\n",
      "Starting epoch: 19 | phase: train | ⏰: 03:45:04\n",
      "Loss: 0.0062 | IoU: 0.6594 | dice: 0.8073 | dice_neg: 0.9284 | dice_pos: 0.7783\n",
      "Starting epoch: 19 | phase: val | ⏰: 03:55:46\n",
      "Loss: 0.0148 | IoU: 0.5251 | dice: 0.7118 | dice_neg: 0.9300 | dice_pos: 0.6567\n",
      "\n",
      "Starting epoch: 20 | phase: train | ⏰: 03:57:26\n",
      "Loss: 0.0063 | IoU: 0.6588 | dice: 0.8068 | dice_neg: 0.9316 | dice_pos: 0.7772\n",
      "Starting epoch: 20 | phase: val | ⏰: 04:08:20\n",
      "Loss: 0.0142 | IoU: 0.5289 | dice: 0.7166 | dice_neg: 0.9402 | dice_pos: 0.6599\n",
      "\n",
      "Starting epoch: 21 | phase: train | ⏰: 04:09:58\n",
      "Loss: 0.0062 | IoU: 0.6611 | dice: 0.8097 | dice_neg: 0.9357 | dice_pos: 0.7795\n",
      "Starting epoch: 21 | phase: val | ⏰: 04:20:42\n",
      "Loss: 0.0147 | IoU: 0.5290 | dice: 0.7145 | dice_neg: 0.9288 | dice_pos: 0.6605\n",
      "Epoch    21: reducing learning rate of group 0 to 5.0000e-08.\n",
      "\n",
      "Starting epoch: 22 | phase: train | ⏰: 04:22:19\n",
      "Loss: 0.0063 | IoU: 0.6611 | dice: 0.8093 | dice_neg: 0.9335 | dice_pos: 0.7793\n",
      "Starting epoch: 22 | phase: val | ⏰: 04:33:15\n",
      "Loss: 0.0152 | IoU: 0.5304 | dice: 0.7182 | dice_neg: 0.9377 | dice_pos: 0.6626\n",
      "\n",
      "Starting epoch: 23 | phase: train | ⏰: 04:34:59\n",
      "Loss: 0.0063 | IoU: 0.6595 | dice: 0.8070 | dice_neg: 0.9309 | dice_pos: 0.7776\n",
      "Starting epoch: 23 | phase: val | ⏰: 04:45:42\n",
      "Loss: 0.0145 | IoU: 0.5291 | dice: 0.7159 | dice_neg: 0.9338 | dice_pos: 0.6595\n",
      "\n",
      "Starting epoch: 24 | phase: train | ⏰: 04:47:23\n",
      "Loss: 0.0064 | IoU: 0.6592 | dice: 0.8063 | dice_neg: 0.9268 | dice_pos: 0.7774\n",
      "Starting epoch: 24 | phase: val | ⏰: 04:58:27\n",
      "Loss: 0.0150 | IoU: 0.5368 | dice: 0.7186 | dice_neg: 0.9148 | dice_pos: 0.6692\n",
      "\n",
      "Starting epoch: 25 | phase: train | ⏰: 05:00:07\n",
      "Loss: 0.0063 | IoU: 0.6607 | dice: 0.8087 | dice_neg: 0.9306 | dice_pos: 0.7789\n",
      "Starting epoch: 25 | phase: val | ⏰: 05:10:57\n",
      "Loss: 0.0156 | IoU: 0.5295 | dice: 0.7159 | dice_neg: 0.9326 | dice_pos: 0.6616\n",
      "Epoch    25: reducing learning rate of group 0 to 5.0000e-09.\n",
      "\n",
      "Starting epoch: 26 | phase: train | ⏰: 05:12:34\n",
      "Loss: 0.0064 | IoU: 0.6604 | dice: 0.8086 | dice_neg: 0.9287 | dice_pos: 0.7789\n",
      "Starting epoch: 26 | phase: val | ⏰: 05:23:16\n",
      "Loss: 0.0143 | IoU: 0.5271 | dice: 0.7068 | dice_neg: 0.9033 | dice_pos: 0.6579\n",
      "\n",
      "Starting epoch: 27 | phase: train | ⏰: 05:24:53\n",
      "Loss: 0.0061 | IoU: 0.6617 | dice: 0.8084 | dice_neg: 0.9297 | dice_pos: 0.7797\n",
      "Starting epoch: 27 | phase: val | ⏰: 05:35:34\n",
      "Loss: 0.0150 | IoU: 0.5343 | dice: 0.7183 | dice_neg: 0.9338 | dice_pos: 0.6646\n",
      "\n",
      "Starting epoch: 28 | phase: train | ⏰: 05:37:11\n",
      "Loss: 0.0063 | IoU: 0.6614 | dice: 0.8108 | dice_neg: 0.9424 | dice_pos: 0.7789\n",
      "Starting epoch: 28 | phase: val | ⏰: 05:47:57\n",
      "Loss: 0.0147 | IoU: 0.5302 | dice: 0.7187 | dice_neg: 0.9491 | dice_pos: 0.6613\n",
      "\n",
      "Starting epoch: 29 | phase: train | ⏰: 05:49:33\n",
      "Loss: 0.0062 | IoU: 0.6586 | dice: 0.8073 | dice_neg: 0.9322 | dice_pos: 0.7772\n",
      "Starting epoch: 29 | phase: val | ⏰: 06:00:14\n",
      "Loss: 0.0153 | IoU: 0.5301 | dice: 0.7152 | dice_neg: 0.9313 | dice_pos: 0.6613\n",
      "\n",
      "Starting epoch: 30 | phase: train | ⏰: 06:01:51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0063 | IoU: 0.6593 | dice: 0.8060 | dice_neg: 0.9274 | dice_pos: 0.7777\n",
      "Starting epoch: 30 | phase: val | ⏰: 06:12:32\n",
      "Loss: 0.0140 | IoU: 0.5267 | dice: 0.7107 | dice_neg: 0.9288 | dice_pos: 0.6567\n",
      "\n",
      "Starting epoch: 31 | phase: train | ⏰: 06:14:09\n",
      "Loss: 0.0062 | IoU: 0.6634 | dice: 0.8102 | dice_neg: 0.9316 | dice_pos: 0.7809\n",
      "Starting epoch: 31 | phase: val | ⏰: 06:24:48\n",
      "Loss: 0.0147 | IoU: 0.5291 | dice: 0.7137 | dice_neg: 0.9262 | dice_pos: 0.6603\n",
      "\n",
      "Starting epoch: 32 | phase: train | ⏰: 06:26:25\n",
      "Loss: 0.0063 | IoU: 0.6624 | dice: 0.8096 | dice_neg: 0.9319 | dice_pos: 0.7806\n",
      "Starting epoch: 32 | phase: val | ⏰: 06:37:08\n",
      "Loss: 0.0152 | IoU: 0.5413 | dice: 0.7219 | dice_neg: 0.9262 | dice_pos: 0.6726\n",
      "\n",
      "Starting epoch: 33 | phase: train | ⏰: 06:38:49\n",
      "Loss: 0.0062 | IoU: 0.6621 | dice: 0.8098 | dice_neg: 0.9306 | dice_pos: 0.7803\n",
      "Starting epoch: 33 | phase: val | ⏰: 06:49:41\n",
      "Loss: 0.0152 | IoU: 0.5269 | dice: 0.7161 | dice_neg: 0.9377 | dice_pos: 0.6587\n",
      "\n",
      "Starting epoch: 34 | phase: train | ⏰: 06:51:18\n",
      "Loss: 0.0063 | IoU: 0.6630 | dice: 0.8085 | dice_neg: 0.9227 | dice_pos: 0.7809\n",
      "Starting epoch: 34 | phase: val | ⏰: 07:02:00\n",
      "Loss: 0.0153 | IoU: 0.5252 | dice: 0.7099 | dice_neg: 0.9148 | dice_pos: 0.6578\n",
      "\n",
      "Starting epoch: 35 | phase: train | ⏰: 07:03:36\n",
      "Loss: 0.0064 | IoU: 0.6607 | dice: 0.8095 | dice_neg: 0.9322 | dice_pos: 0.7796\n",
      "Starting epoch: 35 | phase: val | ⏰: 07:14:18\n",
      "Loss: 0.0149 | IoU: 0.5279 | dice: 0.7117 | dice_neg: 0.9186 | dice_pos: 0.6611\n",
      "\n",
      "Starting epoch: 36 | phase: train | ⏰: 07:15:55\n",
      "Loss: 0.0062 | IoU: 0.6643 | dice: 0.8112 | dice_neg: 0.9303 | dice_pos: 0.7826\n",
      "Starting epoch: 36 | phase: val | ⏰: 07:26:36\n",
      "Loss: 0.0150 | IoU: 0.5258 | dice: 0.7115 | dice_neg: 0.9288 | dice_pos: 0.6580\n",
      "\n",
      "Starting epoch: 37 | phase: train | ⏰: 07:28:13\n",
      "Loss: 0.0061 | IoU: 0.6605 | dice: 0.8088 | dice_neg: 0.9338 | dice_pos: 0.7789\n",
      "Starting epoch: 37 | phase: val | ⏰: 07:38:58\n",
      "Loss: 0.0137 | IoU: 0.5294 | dice: 0.7135 | dice_neg: 0.9288 | dice_pos: 0.6598\n",
      "\n",
      "Starting epoch: 38 | phase: train | ⏰: 07:40:35\n",
      "Loss: 0.0062 | IoU: 0.6638 | dice: 0.8085 | dice_neg: 0.9179 | dice_pos: 0.7817\n",
      "Starting epoch: 38 | phase: val | ⏰: 07:51:15\n",
      "Loss: 0.0152 | IoU: 0.5284 | dice: 0.7124 | dice_neg: 0.9198 | dice_pos: 0.6613\n",
      "\n",
      "Starting epoch: 39 | phase: train | ⏰: 07:52:52\n",
      "Loss: 0.0063 | IoU: 0.6600 | dice: 0.8081 | dice_neg: 0.9341 | dice_pos: 0.7786\n",
      "Starting epoch: 39 | phase: val | ⏰: 08:03:32\n",
      "Loss: 0.0149 | IoU: 0.5256 | dice: 0.7120 | dice_neg: 0.9198 | dice_pos: 0.6572\n",
      "\n",
      "Starting epoch: 40 | phase: train | ⏰: 08:05:09\n",
      "Loss: 0.0063 | IoU: 0.6621 | dice: 0.8114 | dice_neg: 0.9386 | dice_pos: 0.7807\n",
      "Starting epoch: 40 | phase: val | ⏰: 08:15:50\n",
      "Loss: 0.0153 | IoU: 0.5307 | dice: 0.7141 | dice_neg: 0.9249 | dice_pos: 0.6614\n",
      "\n",
      "Starting epoch: 41 | phase: train | ⏰: 08:17:26\n",
      "Loss: 0.0064 | IoU: 0.6615 | dice: 0.8080 | dice_neg: 0.9293 | dice_pos: 0.7794\n",
      "Starting epoch: 41 | phase: val | ⏰: 08:28:09\n",
      "Loss: 0.0149 | IoU: 0.5281 | dice: 0.7110 | dice_neg: 0.9084 | dice_pos: 0.6602\n",
      "\n",
      "Starting epoch: 42 | phase: train | ⏰: 08:29:46\n",
      "Loss: 0.0064 | IoU: 0.6612 | dice: 0.8087 | dice_neg: 0.9325 | dice_pos: 0.7796\n",
      "Starting epoch: 42 | phase: val | ⏰: 08:40:27\n",
      "Loss: 0.0142 | IoU: 0.5264 | dice: 0.7108 | dice_neg: 0.9173 | dice_pos: 0.6589\n",
      "\n",
      "Starting epoch: 43 | phase: train | ⏰: 08:42:03\n",
      "Loss: 0.0062 | IoU: 0.6603 | dice: 0.8087 | dice_neg: 0.9370 | dice_pos: 0.7785\n",
      "Starting epoch: 43 | phase: val | ⏰: 08:52:44\n",
      "Loss: 0.0146 | IoU: 0.5266 | dice: 0.7140 | dice_neg: 0.9364 | dice_pos: 0.6584\n",
      "\n",
      "Starting epoch: 44 | phase: train | ⏰: 08:54:21\n",
      "Loss: 0.0063 | IoU: 0.6609 | dice: 0.8078 | dice_neg: 0.9268 | dice_pos: 0.7796\n",
      "Starting epoch: 44 | phase: val | ⏰: 09:05:01\n",
      "Loss: 0.0141 | IoU: 0.5350 | dice: 0.7173 | dice_neg: 0.9071 | dice_pos: 0.6674\n",
      "\n",
      "Starting epoch: 45 | phase: train | ⏰: 09:06:38\n",
      "Loss: 0.0062 | IoU: 0.6623 | dice: 0.8127 | dice_neg: 0.9449 | dice_pos: 0.7811\n",
      "Starting epoch: 45 | phase: val | ⏰: 09:17:18\n",
      "Loss: 0.0137 | IoU: 0.5339 | dice: 0.7209 | dice_neg: 0.9364 | dice_pos: 0.6653\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 46 | phase: train | ⏰: 09:18:55\n",
      "Loss: 0.0063 | IoU: 0.6616 | dice: 0.8095 | dice_neg: 0.9316 | dice_pos: 0.7798\n",
      "Starting epoch: 46 | phase: val | ⏰: 09:29:58\n",
      "Loss: 0.0142 | IoU: 0.5426 | dice: 0.7246 | dice_neg: 0.9224 | dice_pos: 0.6746\n",
      "\n",
      "Starting epoch: 47 | phase: train | ⏰: 09:31:46\n",
      "Loss: 0.0062 | IoU: 0.6608 | dice: 0.8064 | dice_neg: 0.9236 | dice_pos: 0.7789\n",
      "Starting epoch: 47 | phase: val | ⏰: 09:43:21\n",
      "Loss: 0.0152 | IoU: 0.5256 | dice: 0.7116 | dice_neg: 0.9249 | dice_pos: 0.6566\n",
      "\n",
      "Starting epoch: 48 | phase: train | ⏰: 09:45:14\n",
      "Loss: 0.0063 | IoU: 0.6612 | dice: 0.8098 | dice_neg: 0.9332 | dice_pos: 0.7795\n",
      "Starting epoch: 48 | phase: val | ⏰: 09:56:46\n",
      "Loss: 0.0145 | IoU: 0.5347 | dice: 0.7222 | dice_neg: 0.9389 | dice_pos: 0.6664\n",
      "\n",
      "Starting epoch: 49 | phase: train | ⏰: 09:58:37\n",
      "Loss: 0.0063 | IoU: 0.6610 | dice: 0.8081 | dice_neg: 0.9287 | dice_pos: 0.7795\n",
      "Starting epoch: 49 | phase: val | ⏰: 10:10:04\n",
      "Loss: 0.0154 | IoU: 0.5238 | dice: 0.7143 | dice_neg: 0.9427 | dice_pos: 0.6554\n",
      "\n",
      "Starting epoch: 50 | phase: train | ⏰: 10:11:53\n",
      "Loss: 0.0064 | IoU: 0.6618 | dice: 0.8084 | dice_neg: 0.9313 | dice_pos: 0.7798\n",
      "Starting epoch: 50 | phase: val | ⏰: 10:23:22\n",
      "Loss: 0.0142 | IoU: 0.5258 | dice: 0.7103 | dice_neg: 0.9237 | dice_pos: 0.6570\n",
      "\n",
      "Starting epoch: 51 | phase: train | ⏰: 10:25:13\n",
      "Loss: 0.0062 | IoU: 0.6632 | dice: 0.8115 | dice_neg: 0.9402 | dice_pos: 0.7812\n",
      "Starting epoch: 51 | phase: val | ⏰: 10:36:38\n",
      "Loss: 0.0149 | IoU: 0.5316 | dice: 0.7170 | dice_neg: 0.9300 | dice_pos: 0.6635\n",
      "\n",
      "Starting epoch: 52 | phase: train | ⏰: 10:38:33\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-05f46443fc03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-41800e04cd21>\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;31m#             if epoch+1 == self.num_epochs:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m#                 self.optimizer.swap_swa_sgd()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-41800e04cd21>\u001b[0m in \u001b[0;36miterate\u001b[0;34m(self, epoch, phase)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_trainer = Trainer(model)\n",
    "model_trainer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT TRAINING\n",
    "losses = model_trainer.losses\n",
    "dice_scores = model_trainer.dice_scores # overall dice\n",
    "iou_scores = model_trainer.iou_scores\n",
    "\n",
    "def plot(scores, name):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(range(len(scores[\"train\"])), scores[\"train\"], label=f'train {name}')\n",
    "    plt.plot(range(len(scores[\"train\"])), scores[\"val\"], label=f'val {name}')\n",
    "    plt.title(f'{name} plot'); plt.xlabel('Epoch'); plt.ylabel(f'{name}');\n",
    "    plt.legend(); \n",
    "    plt.show()\n",
    "\n",
    "plot(losses, \"BCE loss\")\n",
    "plot(dice_scores, \"Dice score\")\n",
    "plot(iou_scores, \"IoU score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test prediction and submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training and validation takes about ~400 minutes which exceeds Kaggle's GPU usage limit of 60 minutes, we won't be able to submit the `submission.csv` file generated from this kernel. So, for test prediction and submission I've written a separate [UNet inference kernel](https://www.kaggle.com/rishabhiitbhu/unet-pytorch-inference-kernel), make sure you add the `model.pth` file generated from this kernel as dataset to test inference kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've used resnet-18 architecture in this kernel. It scores ~0.89 on LB. Try to play around with other architectures of `segmenation_models.pytorch` and see what works best for you, let me know in the comments :) and do upvote if you liked this kernel, I need some medals too. 😬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refrences:\n",
    "\n",
    "Few kernels from which I've borrowed some cod[](http://)e:\n",
    "\n",
    "* https://www.kaggle.com/amanooo/defect-detection-starter-u-net\n",
    "* https://www.kaggle.com/go1dfish/clear-mask-visualization-and-simple-eda\n",
    "\n",
    "A big thank you to all those who share their code on Kaggle, I'm nobody without you guys. I've learnt a lot from fellow kagglers, special shout-out to [@Abhishek](https://www.kaggle.com/abhishek), [@Yury](https://www.kaggle.com/deyury), [@Heng](https://www.kaggle.com/hengck23), [@Ekhtiar](https://www.kaggle.com/ekhtiar), [@lafoss](https://www.kaggle.com/iafoss), [@Siddhartha](https://www.kaggle.com/meaninglesslives), [@xhulu](https://www.kaggle.com/xhlulu), and the list goes on.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.4,
   "position": {
    "height": "40px",
    "left": "1266px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
